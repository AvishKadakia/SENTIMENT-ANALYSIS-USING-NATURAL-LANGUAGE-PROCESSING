{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCrDIkgtl98y",
    "outputId": "57598b45-f662-4663-d320-40c5ef2a5765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OrtxHaAwg_ni"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NOsdZnuXhw-r"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dSnsk2bzXq_B"
   },
   "outputs": [],
   "source": [
    "#Dataset Link: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "import requests\n",
    "\n",
    "base_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/\"\n",
    "file_name = \"amazon_reviews_us_Wireless_v1_00\"\n",
    "r = requests.get(base_url+file_name+\".tsv.gz\", allow_redirects=True)\n",
    "#Downlaod data from the server to local machine\n",
    "open(file_name+\".tsv.gz\", 'wb').write(r.content)\n",
    "#Unzip downlaoded data\n",
    "with gzip.open(file_name+\".tsv.gz\", 'rb') as f_in:\n",
    "    with open(file_name+\".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "#Remove old zipped data\n",
    "os.remove(file_name+\".tsv\"+\".gz\")\n",
    "#Store data into a pandas dataframe\n",
    "chunk = pd.read_csv(file_name+\".tsv\", sep='\\t',error_bad_lines=False,chunksize=1000000,low_memory=False)\n",
    "df = pd.concat(chunk)\n",
    "#Remove unzipped file from local machine\n",
    "os.remove(file_name+\".tsv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "print(\"Before cleanning reviews\")\n",
    "print(df[\"review_body\"].head(5))\n",
    "def clean_dataset(X):\n",
    "  review = X\n",
    "  stemmer = WordNetLemmatizer()\n",
    "\n",
    "  # Remove all the special characters\n",
    "  review = re.sub(r'\\W', ' ', review)\n",
    "  \n",
    "  # remove all single characters\n",
    "  review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review)\n",
    "  \n",
    "  # Remove single characters from the start\n",
    "  review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n",
    "  \n",
    "  # Substituting multiple spaces with single space\n",
    "  review = re.sub(r'\\s+', ' ', review, flags=re.I)\n",
    "  \n",
    "  # Removing prefixed 'b'\n",
    "  review = re.sub(r'^b\\s+', '', review)\n",
    "  \n",
    "  # Converting to Lowercase\n",
    "  review = review.lower()\n",
    "  \n",
    "  # Lemmatization\n",
    "  review = review.split()\n",
    "  review = [stemmer.lemmatize(word) for word in review]\n",
    "  review = ' '.join(review)\n",
    "\n",
    "  #Removing all  stopwords.\n",
    "  review = remove_stopwords(review)\n",
    "  return review\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X = df[\"review_body\"].apply(lambda x: clean_dataset(str(x)))\n",
    "print(f\"Total time to clean reviews: {time.time()-start_time}\")\n",
    "print(\"After cleanning reviews\")\n",
    "print(X.head(5))\n",
    "y = df[\"star_rating\"]\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving cleaned data to CSV File\n",
    "X.to_csv(datasetPath+'X_cleaned.csv', index=False) \n",
    "y.to_csv(datasetPath+'y_cleaned.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxZuCRcu9Wxo",
    "outputId": "09a24477-3462-4d36-c749-be2d05b678d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load reviews: 25.63444972038269\n",
      "Target Names: [['Star Rating 0', 'Star Rating 1', 'Star Rating 2', 'Star Rating 3', 'Star Rating 4']]\n",
      "(6013513,)\n",
      "(6013513,)\n",
      "(2961880,)\n",
      "(2961880,)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#Reading saved clean file\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X = pd.read_csv(datasetPath+'X_cleaned.csv')\n",
    "y = pd.read_csv(datasetPath+'y_cleaned.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")\n",
    "\n",
    "#Removing null values\n",
    "result = pd.concat([X,y], axis=1).dropna()\n",
    "X = result[\"review_body\"]\n",
    "y = result[\"star_rating\"]\n",
    "#Cnverting to 5 ratings 0 - 4 from 1 - 5\n",
    "def reset_ratings(r):\n",
    "  if r == 1:\n",
    "    return 0;\n",
    "  if r == 2:\n",
    "    return 1;\n",
    "  if r == 3:\n",
    "    return 2;\n",
    "  if r == 4:\n",
    "    return 3;\n",
    "  if r == 5:\n",
    "    return 4;\n",
    "y = y.apply(lambda x: reset_ratings(int(x)))\n",
    "#Calculating Targets or Classes\n",
    "target_names = []\n",
    "labels = np.unique(y)\n",
    "for label in labels:\n",
    "  target_names.append('Star Rating '+str(label))\n",
    "print(f\"Target Names: {[target_names]}\")\n",
    "#Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0],)\n",
    "X_test = np.array(X_test).reshape(X_test.shape[0],)\n",
    "#Convert all rating to type int\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_test = np.array(y_test).astype(int)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_pEMrLzexAx",
    "outputId": "92838522-242c-43d8-ed63-95dd3d70f436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n",
      "Review 0 of 6013513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 100000 of 6013513\n",
      "Review 200000 of 6013513\n",
      "Review 300000 of 6013513\n",
      "Review 400000 of 6013513\n",
      "Review 500000 of 6013513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 600000 of 6013513\n",
      "Review 700000 of 6013513\n",
      "Review 800000 of 6013513\n",
      "Review 900000 of 6013513\n",
      "Review 1000000 of 6013513\n",
      "Review 1100000 of 6013513\n",
      "Review 1200000 of 6013513\n",
      "Review 1300000 of 6013513\n",
      "Review 1400000 of 6013513\n",
      "Review 1500000 of 6013513\n",
      "Review 1600000 of 6013513\n",
      "Review 1700000 of 6013513\n",
      "Review 1800000 of 6013513\n",
      "Review 1900000 of 6013513\n",
      "Review 2000000 of 6013513\n",
      "Review 2100000 of 6013513\n",
      "Review 2200000 of 6013513\n",
      "Review 2300000 of 6013513\n",
      "Review 2400000 of 6013513\n",
      "Review 2500000 of 6013513\n",
      "Review 2600000 of 6013513\n",
      "Review 2700000 of 6013513\n",
      "Review 2800000 of 6013513\n",
      "Review 2900000 of 6013513\n",
      "Review 3000000 of 6013513\n",
      "Review 3100000 of 6013513\n",
      "Review 3200000 of 6013513\n",
      "Review 3300000 of 6013513\n",
      "Review 3400000 of 6013513\n",
      "Review 3500000 of 6013513\n",
      "Review 3600000 of 6013513\n",
      "Review 3700000 of 6013513\n",
      "Review 3800000 of 6013513\n",
      "Review 3900000 of 6013513\n",
      "Review 4000000 of 6013513\n",
      "Review 4100000 of 6013513\n",
      "Review 4200000 of 6013513\n",
      "Review 4300000 of 6013513\n",
      "Review 4400000 of 6013513\n",
      "Review 4500000 of 6013513\n",
      "Review 4600000 of 6013513\n",
      "Review 4700000 of 6013513\n",
      "Review 4800000 of 6013513\n",
      "Review 4900000 of 6013513\n",
      "Review 5000000 of 6013513\n",
      "Review 5100000 of 6013513\n",
      "Review 5200000 of 6013513\n",
      "Review 5300000 of 6013513\n",
      "Review 5400000 of 6013513\n",
      "Review 5500000 of 6013513\n",
      "Review 5600000 of 6013513\n",
      "Review 5700000 of 6013513\n",
      "Review 5800000 of 6013513\n",
      "Review 5900000 of 6013513\n",
      "Review 6000000 of 6013513\n",
      "Review 0 of 2961880\n",
      "Review 100000 of 2961880\n",
      "Review 200000 of 2961880\n",
      "Review 300000 of 2961880\n",
      "Review 400000 of 2961880\n",
      "Review 500000 of 2961880\n",
      "Review 600000 of 2961880\n",
      "Review 700000 of 2961880\n",
      "Review 800000 of 2961880\n",
      "Review 900000 of 2961880\n",
      "Review 1000000 of 2961880\n",
      "Review 1100000 of 2961880\n",
      "Review 1200000 of 2961880\n",
      "Review 1300000 of 2961880\n",
      "Review 1400000 of 2961880\n",
      "Review 1500000 of 2961880\n",
      "Review 1600000 of 2961880\n",
      "Review 1700000 of 2961880\n",
      "Review 1800000 of 2961880\n",
      "Review 1900000 of 2961880\n",
      "Review 2000000 of 2961880\n",
      "Review 2100000 of 2961880\n",
      "Review 2200000 of 2961880\n",
      "Review 2300000 of 2961880\n",
      "Review 2400000 of 2961880\n",
      "Review 2500000 of 2961880\n",
      "Review 2600000 of 2961880\n",
      "Review 2700000 of 2961880\n",
      "Review 2800000 of 2961880\n",
      "Review 2900000 of 2961880\n",
      "Total time for word2 vec: 7543.71201133728\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "start_time = time.time()\n",
    "model = word2vec.Word2Vec(X_train,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 100000th review\n",
    "        if counter%100000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "X_train = getAvgFeatureVecs(X_train, model, num_features)\n",
    "X_test = getAvgFeatureVecs(X_test, model, num_features)\n",
    "print(f\"Total time for word2 vec: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VEUQq1LChH1c"
   },
   "outputs": [],
   "source": [
    "#scaling vector between 0 - 1 \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train =scaler.transform(X_train)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tZ_U94n3Ltx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_F81_dEVS49",
    "outputId": "b5af1c5c-f998-4d8f-84c8-d5d8a5337b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving to CSV File\n",
    "np.savetxt(datasetPath+'X_train_w2v.csv', X_train, delimiter=\",\") \n",
    "np.savetxt(datasetPath+'X_test_w2v.csv', X_test, delimiter=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7mLfGe1hKg_",
    "outputId": "a36749b4-018b-4f38-f435-b23a36b9a4d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Total time to load reviews: 383.30136036872864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X_train_temp = pd.read_csv(datasetPath+'X_train_w2v.csv')\n",
    "X_test_temp = pd.read_csv(datasetPath+'X_test_w2v.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Jq_MjY-tVzT8"
   },
   "outputs": [],
   "source": [
    "X_train_cleaned = X_train[~np.isnan(X_train)]\n",
    "X_test_cleaned = X_test[~np.isnan(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QfIR-Xffa6Qj"
   },
   "outputs": [],
   "source": [
    "X_train_cleaned= X_train_cleaned.reshape(-1,100)\n",
    "X_test_cleaned = X_test_cleaned.reshape(-1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EVNDo3J81w9O"
   },
   "outputs": [],
   "source": [
    "y_train_cleaned = y_train[0:-1]\n",
    "y_test_cleaned = y_test[0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9Pgu17_ZNPt",
    "outputId": "f4b52c2c-67dd-43d0-cbfd-9135374f8205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using SVM\n",
      "Batch 0 : 601351 of 6013512 : 2.8775529861450195 seconds \n",
      "Batch 1 : 1202702 of 6013512 : 2.7181010246276855 seconds \n",
      "Batch 2 : 1804053 of 6013512 : 2.668421506881714 seconds \n",
      "Batch 3 : 2405404 of 6013512 : 2.732713460922241 seconds \n",
      "Batch 4 : 3006755 of 6013512 : 2.7584621906280518 seconds \n",
      "Batch 5 : 3608106 of 6013512 : 2.7354469299316406 seconds \n",
      "Batch 6 : 4209457 of 6013512 : 2.6530075073242188 seconds \n",
      "Batch 7 : 4810808 of 6013512 : 2.698456048965454 seconds \n",
      "Batch 8 : 5412159 of 6013512 : 2.6358695030212402 seconds \n",
      "Batch 9 : 6013510 of 6013512 : 2.6524319648742676 seconds \n",
      "Total training time: 27.132729053497314 \n",
      "Test dataset accuracy for classifier is 0.5355581830176664 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.00      0.00      0.00    415969\n",
      "Star Rating 1       0.00      0.00      0.00    196705\n",
      "Star Rating 2       0.00      0.00      0.00    268689\n",
      "Star Rating 3       0.00      0.00      0.00    494257\n",
      "Star Rating 4       0.54      1.00      0.70   1586258\n",
      "\n",
      "     accuracy                           0.54   2961878\n",
      "    macro avg       0.11      0.20      0.14   2961878\n",
      " weighted avg       0.29      0.54      0.37   2961878\n",
      "\n",
      "Total time to test: 9.36195969581604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "n_batches = 10\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train_cleaned)/n_batches))\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "\n",
    "print(f\"Classifying using SVM\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train_cleaned[current_batch:next_batch], y_train_cleaned[current_batch:next_batch], classes=labels)\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train_cleaned)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "svm_accuracy = accuracy_score(y_test_cleaned, clf.predict(X_test_cleaned))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test_cleaned, clf.predict(X_test_cleaned), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKKEV3TKoXXx",
    "outputId": "ba5b700f-1b5b-4e31-d869-6843e17a3ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using multinomialNB\n",
      "Total time to train: 8.080581426620483\n",
      "Test dataset accuracy is 0.5355581830176664 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.00      0.00      0.00    415969\n",
      "Star Rating 1       0.00      0.00      0.00    196705\n",
      "Star Rating 2       0.00      0.00      0.00    268689\n",
      "Star Rating 3       0.00      0.00      0.00    494257\n",
      "Star Rating 4       0.54      1.00      0.70   1586258\n",
      "\n",
      "     accuracy                           0.54   2961878\n",
      "    macro avg       0.11      0.20      0.14   2961878\n",
      " weighted avg       0.29      0.54      0.37   2961878\n",
      "\n",
      "Total time to test: 9.111035108566284\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Classifying using multinomialNB\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_cleaned, y_train_cleaned)\n",
    "print(f\"Total time to train: {time.time()-start_time}\")\n",
    "start_time = time.time()\n",
    "nb_accuracy = accuracy_score(y_test_cleaned, clf.predict(X_test_cleaned))\n",
    "print(f\"Test dataset accuracy is {nb_accuracy} \")\n",
    "print(classification_report(y_test_cleaned, clf.predict(X_test_cleaned), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOePhgzCGhXy",
    "outputId": "4195bf94-3b94-4e83-99f1-c0c7bbdb89b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using KNN\n",
      "Batch 0 : 6013512 of 6013512 : 31.426869869232178 seconds \n",
      "Total training time: 31.42732548713684 \n",
      "Test dataset accuracy for classifier is 0.5355581830176664 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.13      0.01      0.02    415969\n",
      "Star Rating 1       0.07      0.54      0.13    196705\n",
      "Star Rating 2       0.09      0.02      0.03    268689\n",
      "Star Rating 3       0.13      0.02      0.04    494257\n",
      "Star Rating 4       0.55      0.46      0.50   1586258\n",
      "\n",
      "     accuracy                           0.29   2961878\n",
      "    macro avg       0.19      0.21      0.14   2961878\n",
      " weighted avg       0.35      0.29      0.29   2961878\n",
      "\n",
      "Total time to test: 2770.8097462654114\n"
     ]
    }
   ],
   "source": [
    "n_batches = 1\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train_cleaned)/n_batches))\n",
    "clf = MiniBatchKMeans(n_clusters=len(labels),random_state=0,batch_size=n_batches)\n",
    "\n",
    "print(f\"Classifying using KNN\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train_cleaned[current_batch:next_batch])\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train_cleaned)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "knn_accuracy = accuracy_score(y_test_cleaned, clf.predict(X_test_cleaned))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test_cleaned, clf.predict(X_test_cleaned), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_Project_Word2Vec_Classes_5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
