{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCrDIkgtl98y",
    "outputId": "dccb7e32-151c-4ad5-a870-64853728fa1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OrtxHaAwg_ni"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qRJhSCDLbHKO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NOsdZnuXhw-r"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dSnsk2bzXq_B"
   },
   "outputs": [],
   "source": [
    "#Dataset Link: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "import requests\n",
    "\n",
    "base_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/\"\n",
    "file_name = \"amazon_reviews_us_Wireless_v1_00\"\n",
    "r = requests.get(base_url+file_name+\".tsv.gz\", allow_redirects=True)\n",
    "#Downlaod data from the server to local machine\n",
    "open(file_name+\".tsv.gz\", 'wb').write(r.content)\n",
    "#Unzip downlaoded data\n",
    "with gzip.open(file_name+\".tsv.gz\", 'rb') as f_in:\n",
    "    with open(file_name+\".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "#Remove old zipped data\n",
    "os.remove(file_name+\".tsv\"+\".gz\")\n",
    "#Store data into a pandas dataframe\n",
    "chunk = pd.read_csv(file_name+\".tsv\", sep='\\t',error_bad_lines=False,chunksize=1000000,low_memory=False)\n",
    "df = pd.concat(chunk)\n",
    "#Remove unzipped file from local machine\n",
    "os.remove(file_name+\".tsv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "print(\"Before cleanning reviews\")\n",
    "print(df[\"review_body\"].head(5))\n",
    "def clean_dataset(X):\n",
    "  review = X\n",
    "  stemmer = WordNetLemmatizer()\n",
    "\n",
    "  # Remove all the special characters\n",
    "  review = re.sub(r'\\W', ' ', review)\n",
    "  \n",
    "  # remove all single characters\n",
    "  review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review)\n",
    "  \n",
    "  # Remove single characters from the start\n",
    "  review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n",
    "  \n",
    "  # Substituting multiple spaces with single space\n",
    "  review = re.sub(r'\\s+', ' ', review, flags=re.I)\n",
    "  \n",
    "  # Removing prefixed 'b'\n",
    "  review = re.sub(r'^b\\s+', '', review)\n",
    "  \n",
    "  # Converting to Lowercase\n",
    "  review = review.lower()\n",
    "  \n",
    "  # Lemmatization\n",
    "  review = review.split()\n",
    "  review = [stemmer.lemmatize(word) for word in review]\n",
    "  review = ' '.join(review)\n",
    "\n",
    "  #Removing all  stopwords.\n",
    "  review = remove_stopwords(review)\n",
    "  return review\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X = df[\"review_body\"].apply(lambda x: clean_dataset(str(x)))\n",
    "print(f\"Total time to clean reviews: {time.time()-start_time}\")\n",
    "print(\"After cleanning reviews\")\n",
    "print(X.head(5))\n",
    "y = df[\"star_rating\"]\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving cleaned data to CSV File\n",
    "X.to_csv(datasetPath+'X_cleaned.csv', index=False) \n",
    "y.to_csv(datasetPath+'y_cleaned.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxZuCRcu9Wxo",
    "outputId": "4ce16474-e420-4b70-d146-293fae0e1ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load reviews: 27.96455955505371\n",
      "Target Names: [['Star Rating 0', 'Star Rating 1', 'Star Rating 2']]\n",
      "(6013513,)\n",
      "(6013513,)\n",
      "(2961880,)\n",
      "(2961880,)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#Reading saved clean file\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X = pd.read_csv(datasetPath+'X_cleaned.csv')\n",
    "y = pd.read_csv(datasetPath+'y_cleaned.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")\n",
    "\n",
    "#Removing null values\n",
    "result = pd.concat([X,y], axis=1).dropna()\n",
    "X = result[\"review_body\"]\n",
    "y = result[\"star_rating\"]\n",
    "#Cnverting to 5 ratings 0 - 4 from 1 - 5\n",
    "def reset_ratings(r):\n",
    "  if r == 1:\n",
    "    return 0;\n",
    "  if r == 2:\n",
    "    return 0;\n",
    "  if r == 3:\n",
    "    return 1;\n",
    "  if r == 4:\n",
    "    return 2;\n",
    "  if r == 5:\n",
    "    return 2;\n",
    "y = y.apply(lambda x: reset_ratings(int(x)))\n",
    "#Calculating Targets or Classes\n",
    "target_names = []\n",
    "labels = np.unique(y)\n",
    "for label in labels:\n",
    "  target_names.append('Star Rating '+str(label))\n",
    "print(f\"Target Names: {[target_names]}\")\n",
    "#Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0],)\n",
    "X_test = np.array(X_test).reshape(X_test.shape[0],)\n",
    "#Convert all rating to type int\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_test = np.array(y_test).astype(int)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W_pEMrLzexAx"
   },
   "outputs": [],
   "source": [
    "def transform(array):    \n",
    "    vectorizer = CountVectorizer(max_features=100, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "    vectorized = vectorizer.fit_transform(array)\n",
    "    transformer = TfidfTransformer()\n",
    "    transformed = transformer.fit_transform(vectorized)\n",
    "    return np.array(transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8f-H97-aErw",
    "outputId": "c2adf813-8e76-49b7-bfda-b51eedc68a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Count Vectors for training data\n",
      "(6013513, 100)\n",
      "Creating Count Vectors for testing data\n",
      "(2961880, 100)\n",
      "Total time to vectorize: 250.31044507026672\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Creating Count Vectors for training data\")\n",
    "X_train = transform(X_train)\n",
    "print(np.array(X_train).shape)\n",
    "print(\"Creating Count Vectors for testing data\")\n",
    "X_test = transform(X_test)\n",
    "print(np.array(X_test).shape)\n",
    "print(f\"Total time to vectorize: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riJC3XhlaIkJ",
    "outputId": "70cd2695-3128-4fe0-efa6-bc5b39f38048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting sparse matrix to dense matrix\n",
      "Total time to vectorize: 9.82465386390686\n"
     ]
    }
   ],
   "source": [
    "# convert to sparse matrix to dense matrix\n",
    "start_time = time.time()\n",
    "print(\"Converting sparse matrix to dense matrix\")\n",
    "X_train = csr_matrix(X_train)\n",
    "X_test = csr_matrix(X_test)\n",
    "print(f\"Total time to vectorize: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VEUQq1LChH1c"
   },
   "outputs": [],
   "source": [
    "# #scaling vector between 0 - 1 \n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train =scaler.transform(X_train)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_test)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "X_test= X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tZ_U94n3Ltx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9Pgu17_ZNPt",
    "outputId": "a48bca8d-be77-4771-f805-fbf905ab954b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using SVM\n",
      "Batch 0 : 601351 of 6013513 : 1.3505747318267822 seconds \n",
      "Batch 1 : 1202702 of 6013513 : 1.4387307167053223 seconds \n",
      "Batch 2 : 1804053 of 6013513 : 1.3935582637786865 seconds \n",
      "Batch 3 : 2405404 of 6013513 : 1.2907111644744873 seconds \n",
      "Batch 4 : 3006755 of 6013513 : 1.35233736038208 seconds \n",
      "Batch 5 : 3608106 of 6013513 : 1.3041882514953613 seconds \n",
      "Batch 6 : 4209457 of 6013513 : 1.319347620010376 seconds \n",
      "Batch 7 : 4810808 of 6013513 : 1.393507719039917 seconds \n",
      "Batch 8 : 5412159 of 6013513 : 1.4104468822479248 seconds \n",
      "Batch 9 : 6013510 of 6013513 : 1.3850500583648682 seconds \n",
      "Total training time: 13.641655921936035 \n",
      "Test dataset accuracy for classifier is 0.6721072426972058 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.35      0.24      0.29    612674\n",
      "Star Rating 1       0.00      0.00      0.00    268689\n",
      "Star Rating 2       0.72      0.89      0.80   2080517\n",
      "\n",
      "     accuracy                           0.67   2961880\n",
      "    macro avg       0.36      0.38      0.36   2961880\n",
      " weighted avg       0.58      0.67      0.62   2961880\n",
      "\n",
      "Total time to test: 7.6421709060668945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "n_batches = 10\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train)/n_batches))\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "\n",
    "print(f\"Classifying using SVM\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train[current_batch:next_batch], y_train[current_batch:next_batch], classes=labels)\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "svm_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKKEV3TKoXXx",
    "outputId": "c80dc985-3c24-4600-9511-4b7d01a44170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using multinomialNB\n",
      "Total time to train: 3.102656126022339\n",
      "Test dataset accuracy is 0.6926685078396154 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.36      0.07      0.12    612674\n",
      "Star Rating 1       0.00      0.00      0.00    268689\n",
      "Star Rating 2       0.71      0.96      0.82   2080517\n",
      "\n",
      "     accuracy                           0.69   2961880\n",
      "    macro avg       0.35      0.35      0.31   2961880\n",
      " weighted avg       0.57      0.69      0.60   2961880\n",
      "\n",
      "Total time to test: 7.187659502029419\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Classifying using multinomialNB\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Total time to train: {time.time()-start_time}\")\n",
    "start_time = time.time()\n",
    "nb_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy is {nb_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOePhgzCGhXy",
    "outputId": "4018200f-3b98-4558-b287-78fc45fb8cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using KNN\n",
      "Batch 0 : 6013513 of 6013513 : 15.1807279586792 seconds \n",
      "Total training time: 15.181747436523438 \n",
      "Test dataset accuracy for classifier is 0.35008102961632476 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.23      0.61      0.33    612674\n",
      "Star Rating 1       0.07      0.11      0.08    268689\n",
      "Star Rating 2       0.72      0.30      0.43   2080517\n",
      "\n",
      "     accuracy                           0.35   2961880\n",
      "    macro avg       0.34      0.34      0.28   2961880\n",
      " weighted avg       0.56      0.35      0.38   2961880\n",
      "\n",
      "Total time to test: 2103.579432487488\n"
     ]
    }
   ],
   "source": [
    "n_batches = 1\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train)/n_batches))\n",
    "clf = MiniBatchKMeans(n_clusters=len(labels),random_state=0,batch_size=n_batches)\n",
    "\n",
    "print(f\"Classifying using KNN\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train[current_batch:next_batch])\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "knn_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy for classifier is {knn_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PR_Project_CountVector_3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
