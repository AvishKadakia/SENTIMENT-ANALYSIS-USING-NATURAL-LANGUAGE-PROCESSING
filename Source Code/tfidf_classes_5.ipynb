{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCrDIkgtl98y",
    "outputId": "74f579ed-194a-4e4e-f7fe-a2ae0f3f028e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OrtxHaAwg_ni"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NOsdZnuXhw-r"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dSnsk2bzXq_B"
   },
   "outputs": [],
   "source": [
    "#https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "import requests\n",
    "base_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/\"\n",
    "file_name = \"amazon_reviews_us_Wireless_v1_00\"\n",
    "r = requests.get(base_url+file_name+\".tsv.gz\", allow_redirects=True)\n",
    "open(file_name+\".tsv.gz\", 'wb').write(r.content)\n",
    "\n",
    "with gzip.open(file_name+\".tsv.gz\", 'rb') as f_in:\n",
    "    with open(file_name+\".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "os.remove(file_name+\".tsv\"+\".gz\")\n",
    "chunk = pd.read_csv(file_name+\".tsv\", sep='\\t',error_bad_lines=False,chunksize=1000000,low_memory=False)\n",
    "df = pd.concat(chunk)\n",
    "#Removing null values\n",
    "df = df.dropna()\n",
    "os.remove(file_name+\".tsv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "print(\"Before cleanning reviews\")\n",
    "print(df[\"review_body\"].head(5))\n",
    "def clean_dataset(X):\n",
    "  review = X\n",
    "  stemmer = WordNetLemmatizer()\n",
    "\n",
    "  # Remove all the special characters\n",
    "  review = re.sub(r'\\W', ' ', review)\n",
    "  \n",
    "  # remove all single characters\n",
    "  review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review)\n",
    "  \n",
    "  # Remove single characters from the start\n",
    "  review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n",
    "  \n",
    "  # Substituting multiple spaces with single space\n",
    "  review = re.sub(r'\\s+', ' ', review, flags=re.I)\n",
    "  \n",
    "  # Removing prefixed 'b'\n",
    "  review = re.sub(r'^b\\s+', '', review)\n",
    "  \n",
    "  # Converting to Lowercase\n",
    "  review = review.lower()\n",
    "  \n",
    "  # Lemmatization\n",
    "  review = review.split()\n",
    "  review = [stemmer.lemmatize(word) for word in review]\n",
    "  review = ' '.join(review)\n",
    "\n",
    "  #Removing all  stopwords.\n",
    "  review = remove_stopwords(review)\n",
    "  return review\n",
    "\n",
    "start_time = time.time()\n",
    "def combined_features(row):\n",
    "    return row['product_title'] + ' '+row['review_headline']+' '+ row['review_body']\n",
    "\n",
    "X = df.apply(combined_features, axis=1)\n",
    "\n",
    "X = df[\"review_body\"].apply(lambda x: clean_dataset(str(x)))\n",
    "print(f\"Total time to clean reviews: {time.time()-start_time}\")\n",
    "print(\"After cleanning reviews\")\n",
    "print(X.head(5))\n",
    "y = df[\"star_rating\"]\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving to CSV File\n",
    "X.to_csv(datasetPath+'X_cleaned.csv', index=False) \n",
    "y.to_csv(datasetPath+'y_cleaned.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxZuCRcu9Wxo",
    "outputId": "409006c0-0ebb-4c5a-ad3e-034d49e2a9f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load reviews: 27.463109016418457\n",
      "(6013513,)\n",
      "(6013513,)\n",
      "(2961880,)\n",
      "(2961880,)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X = pd.read_csv(datasetPath+'X_cleaned.csv')\n",
    "y = pd.read_csv(datasetPath+'y_cleaned.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")\n",
    "\n",
    "#Removing null values\n",
    "result = pd.concat([X,y], axis=1).dropna()\n",
    "result.head(10)\n",
    "X = result[\"review_body\"]\n",
    "y = result[\"star_rating\"]\n",
    "\n",
    "#Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0],)\n",
    "X_test = np.array(X_test).reshape(X_test.shape[0],)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_pEMrLzexAx",
    "outputId": "99c15b58-e352-4c5e-fa21-15cad9302173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual number of tfidf features: (6013513, 1000)\n",
      "\n",
      "Performing dimensionality reduction using LSA\n",
      "  done in 373.250sec\n"
     ]
    }
   ],
   "source": [
    "# Tfidf vectorizer:\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, max_features=1000,\n",
    "                             min_df=5, stop_words='english',\n",
    "                             use_idf=True)\n",
    "\n",
    "# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n",
    "# (\"transform\").\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(f\"  Actual number of tfidf features: {X_train_tfidf.get_shape()}\")\n",
    "\n",
    "print(\"\\nPerforming dimensionality reduction using LSA\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Project the tfidf vectors onto the first N principal components.\n",
    "# Though this is significantly fewer features than the original tfidf vector,\n",
    "# they are stronger features, and the accuracy is higher.\n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))\n",
    "\n",
    "# explained_variance = svd.explained_variance_ratio_.sum()\n",
    "# print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VEUQq1LChH1c"
   },
   "outputs": [],
   "source": [
    "#scaling vector between 0 - 1 \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_lsa)\n",
    "X_train_lsa_scaled =scaler.transform(X_train_lsa)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_test_lsa)\n",
    "X_test_lsa_scaled =scaler.transform(X_test_lsa)\n",
    "\n",
    "#Convert all rating to type int\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_test = np.array(y_test).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L1OJYaXstYED"
   },
   "outputs": [],
   "source": [
    "y_test_old = y_test \n",
    "y_train_old = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSlYeKqhUI8j",
    "outputId": "fb22562a-901a-4601-ca7c-b78ccb67a75e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test_old \n",
    "y_train = y_train_old\n",
    "#Using only 3 ratings:\n",
    "def rating(r):\n",
    "  if r == 1:\n",
    "    return 0;\n",
    "  if r == 2:\n",
    "    return 1;\n",
    "  if r == 3:\n",
    "    return 2;\n",
    "  if r == 4:\n",
    "    return 3;\n",
    "  if r == 5:\n",
    "    return 4;\n",
    "ratings = lambda t: rating(t)\n",
    "vfunc = np.vectorize(ratings)\n",
    "y_train = vfunc(y_train)\n",
    "y_test = vfunc(y_test)\n",
    "\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j97n8AyVlJSF",
    "outputId": "48c5ef17-7053-4292-cb91-87261870a0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Names: [['Star Rating 0', 'Star Rating 1', 'Star Rating 2', 'Star Rating 3', 'Star Rating 4']]\n"
     ]
    }
   ],
   "source": [
    "target_names = []\n",
    "labels = np.unique(y_test)\n",
    "for label in labels:\n",
    "  target_names.append('Star Rating '+str(label))\n",
    "print(f\"Target Names: {[target_names]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9Pgu17_ZNPt",
    "outputId": "1415adc2-5b0b-448c-9f28-2e8a9798c6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using SVM\n",
      "Batch 0 : 601351 of 6013513 : 2.013427495956421 seconds \n",
      "Batch 1 : 1202702 of 6013513 : 1.9768202304840088 seconds \n",
      "Batch 2 : 1804053 of 6013513 : 1.975494384765625 seconds \n",
      "Batch 3 : 2405404 of 6013513 : 2.423121452331543 seconds \n",
      "Batch 4 : 3006755 of 6013513 : 1.970905065536499 seconds \n",
      "Batch 5 : 3608106 of 6013513 : 2.004537582397461 seconds \n",
      "Batch 6 : 4209457 of 6013513 : 1.9648942947387695 seconds \n",
      "Batch 7 : 4810808 of 6013513 : 2.000108242034912 seconds \n",
      "Batch 8 : 5412159 of 6013513 : 1.9616999626159668 seconds \n",
      "Batch 9 : 6013510 of 6013513 : 1.9842031002044678 seconds \n",
      "Total training time: 20.27700448036194 \n",
      "Test dataset accuracy for classifier is 0.6064688643699272 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.54      0.58      0.56    415969\n",
      "Star Rating 1       0.24      0.00      0.00    196705\n",
      "Star Rating 2       0.39      0.04      0.07    268689\n",
      "Star Rating 3       0.32      0.04      0.07    494257\n",
      "Star Rating 4       0.63      0.96      0.76   1586260\n",
      "\n",
      "     accuracy                           0.61   2961880\n",
      "    macro avg       0.42      0.32      0.29   2961880\n",
      " weighted avg       0.52      0.61      0.50   2961880\n",
      "\n",
      "Total time to test: 8.26319932937622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "n_batches = 10\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train_lsa_scaled)/n_batches))\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "\n",
    "print(f\"Classifying using SVM\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train_lsa_scaled[current_batch:next_batch], y_train[current_batch:next_batch], classes=labels)\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train_lsa_scaled)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "svm_accuracy = accuracy_score(y_test, clf.predict(X_test_lsa_scaled))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test_lsa_scaled), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKKEV3TKoXXx",
    "outputId": "733746af-41d3-4398-b414-d74c0e0ebdae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using multinomialNB\n",
      "Total time to train: 3.4640023708343506\n",
      "Test dataset accuracy is 0.5355584966305185 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.00      0.00      0.00    415969\n",
      "Star Rating 1       0.00      0.00      0.00    196705\n",
      "Star Rating 2       0.00      0.00      0.00    268689\n",
      "Star Rating 3       0.00      0.00      0.00    494257\n",
      "Star Rating 4       0.54      1.00      0.70   1586260\n",
      "\n",
      "     accuracy                           0.54   2961880\n",
      "    macro avg       0.11      0.20      0.14   2961880\n",
      " weighted avg       0.29      0.54      0.37   2961880\n",
      "\n",
      "Total time to test: 7.6708197593688965\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Classifying using multinomialNB\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_lsa_scaled, y_train)\n",
    "print(f\"Total time to train: {time.time()-start_time}\")\n",
    "start_time = time.time()\n",
    "nb_accuracy = accuracy_score(y_test, clf.predict(X_test_lsa_scaled))\n",
    "print(f\"Test dataset accuracy is {nb_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test_lsa_scaled), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOePhgzCGhXy",
    "outputId": "424490c2-bd95-409a-aca8-55239b4a8988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using KNN\n",
      "Batch 0 : 6013513 of 6013513 : 18.13900923728943 seconds \n",
      "Total training time: 18.139588594436646 \n",
      "Test dataset accuracy for classifier is 0.6064688643699272 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.24      0.23      0.24    415969\n",
      "Star Rating 1       0.06      0.16      0.09    196705\n",
      "Star Rating 2       0.08      0.23      0.12    268689\n",
      "Star Rating 3       0.12      0.13      0.12    494257\n",
      "Star Rating 4       0.48      0.25      0.32   1586260\n",
      "\n",
      "     accuracy                           0.22   2961880\n",
      "    macro avg       0.20      0.20      0.18   2961880\n",
      " weighted avg       0.32      0.22      0.24   2961880\n",
      "\n",
      "Total time to test: 2345.685484647751\n"
     ]
    }
   ],
   "source": [
    "n_batches = 1\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train_lsa_scaled)/n_batches))\n",
    "clf = MiniBatchKMeans(n_clusters=len(labels),random_state=0,batch_size=n_batches)\n",
    "\n",
    "print(f\"Classifying using KNN\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train_lsa_scaled[current_batch:next_batch])\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train_lsa_scaled)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "knn_accuracy = accuracy_score(y_test, clf.predict(X_test_lsa_scaled))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test_lsa_scaled), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_Project_TFiDf_Classes_5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
