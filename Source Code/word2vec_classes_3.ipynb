{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCrDIkgtl98y",
    "outputId": "9c34fbce-084d-4884-b735-cc41ed5b6408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OrtxHaAwg_ni"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NOsdZnuXhw-r"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dSnsk2bzXq_B"
   },
   "outputs": [],
   "source": [
    "#Dataset Link: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "import requests\n",
    "\n",
    "base_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/\"\n",
    "file_name = \"amazon_reviews_us_Wireless_v1_00\"\n",
    "r = requests.get(base_url+file_name+\".tsv.gz\", allow_redirects=True)\n",
    "#Downlaod data from the server to local machine\n",
    "open(file_name+\".tsv.gz\", 'wb').write(r.content)\n",
    "#Unzip downlaoded data\n",
    "with gzip.open(file_name+\".tsv.gz\", 'rb') as f_in:\n",
    "    with open(file_name+\".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "#Remove old zipped data\n",
    "os.remove(file_name+\".tsv\"+\".gz\")\n",
    "#Store data into a pandas dataframe\n",
    "chunk = pd.read_csv(file_name+\".tsv\", sep='\\t',error_bad_lines=False,chunksize=1000000,low_memory=False)\n",
    "df = pd.concat(chunk)\n",
    "#Remove unzipped file from local machine\n",
    "os.remove(file_name+\".tsv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "print(\"Before cleanning reviews\")\n",
    "print(df[\"review_body\"].head(5))\n",
    "def clean_dataset(X):\n",
    "  review = X\n",
    "  stemmer = WordNetLemmatizer()\n",
    "\n",
    "  # Remove all the special characters\n",
    "  review = re.sub(r'\\W', ' ', review)\n",
    "  \n",
    "  # remove all single characters\n",
    "  review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review)\n",
    "  \n",
    "  # Remove single characters from the start\n",
    "  review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n",
    "  \n",
    "  # Substituting multiple spaces with single space\n",
    "  review = re.sub(r'\\s+', ' ', review, flags=re.I)\n",
    "  \n",
    "  # Removing prefixed 'b'\n",
    "  review = re.sub(r'^b\\s+', '', review)\n",
    "  \n",
    "  # Converting to Lowercase\n",
    "  review = review.lower()\n",
    "  \n",
    "  # Lemmatization\n",
    "  review = review.split()\n",
    "  review = [stemmer.lemmatize(word) for word in review]\n",
    "  review = ' '.join(review)\n",
    "\n",
    "  #Removing all  stopwords.\n",
    "  review = remove_stopwords(review)\n",
    "  return review\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X = df[\"review_body\"].apply(lambda x: clean_dataset(str(x)))\n",
    "print(f\"Total time to clean reviews: {time.time()-start_time}\")\n",
    "print(\"After cleanning reviews\")\n",
    "print(X.head(5))\n",
    "y = df[\"star_rating\"]\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving cleaned data to CSV File\n",
    "X.to_csv(datasetPath+'X_cleaned.csv', index=False) \n",
    "y.to_csv(datasetPath+'y_cleaned.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxZuCRcu9Wxo",
    "outputId": "5d6a1dff-68c6-4f82-86f7-afa2e5d54b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load reviews: 27.427425384521484\n",
      "Target Names: [['Star Rating 0', 'Star Rating 1', 'Star Rating 2']]\n",
      "(6013513,)\n",
      "(6013513,)\n",
      "(2961880,)\n",
      "(2961880,)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#Reading saved clean file\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X = pd.read_csv(datasetPath+'X_cleaned.csv')\n",
    "y = pd.read_csv(datasetPath+'y_cleaned.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")\n",
    "\n",
    "#Removing null values\n",
    "result = pd.concat([X,y], axis=1).dropna()\n",
    "X = result[\"review_body\"]\n",
    "y = result[\"star_rating\"]\n",
    "#Cnverting to 5 ratings 0 - 4 from 1 - 5\n",
    "def reset_ratings(r):\n",
    "  if r == 1:\n",
    "    return 0;\n",
    "  if r == 2:\n",
    "    return 0;\n",
    "  if r == 3:\n",
    "    return 1;\n",
    "  if r == 4:\n",
    "    return 2;\n",
    "  if r == 5:\n",
    "    return 2;\n",
    "y = y.apply(lambda x: reset_ratings(int(x)))\n",
    "#Calculating Targets or Classes\n",
    "target_names = []\n",
    "labels = np.unique(y)\n",
    "for label in labels:\n",
    "  target_names.append('Star Rating '+str(label))\n",
    "print(f\"Target Names: {[target_names]}\")\n",
    "#Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0],)\n",
    "X_test = np.array(X_test).reshape(X_test.shape[0],)\n",
    "#Convert all rating to type int\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_test = np.array(y_test).astype(int)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "W_pEMrLzexAx"
   },
   "outputs": [],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "start_time = time.time()\n",
    "model = word2vec.Word2Vec(X_train,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 100000th review\n",
    "        if counter%100000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "X_train = getAvgFeatureVecs(X_train, model, num_features)\n",
    "X_test = getAvgFeatureVecs(X_test, model, num_features)\n",
    "print(f\"Total time for word2 vec: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VEUQq1LChH1c"
   },
   "outputs": [],
   "source": [
    "#scaling vector between 0 - 1 \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train =scaler.transform(X_train)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tZ_U94n3Ltx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A_F81_dEVS49"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving to CSV File\n",
    "np.savetxt(datasetPath+'X_train_w2v.csv', X_train, delimiter=\",\") \n",
    "np.savetxt(datasetPath+'X_test_w2v.csv', X_test, delimiter=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7mLfGe1hKg_",
    "outputId": "67a27697-2604-4f0e-9bb1-d6459dce3be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Total time to load reviews: 370.01108026504517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X_train = pd.read_csv(datasetPath+'X_train_w2v.csv')\n",
    "X_test = pd.read_csv(datasetPath+'X_test_w2v.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HgV1RloeAgmr"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Jq_MjY-tVzT8"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[~np.isnan(X_train)]\n",
    "X_test = X_test[~np.isnan(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QfIR-Xffa6Qj"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).reshape(-1,100)\n",
    "X_test= np.array(X_test).reshape(-1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EVNDo3J81w9O"
   },
   "outputs": [],
   "source": [
    "y_train = y_train[0:-2]\n",
    "y_test = y_test[0:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9Pgu17_ZNPt",
    "outputId": "44847f48-db69-4d7e-9538-46942d598cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using SVM\n",
      "Batch 0 : 601351 of 6013511 : 1.5657932758331299 seconds \n",
      "Batch 1 : 1202702 of 6013511 : 1.4991331100463867 seconds \n",
      "Batch 2 : 1804053 of 6013511 : 1.4610180854797363 seconds \n",
      "Batch 3 : 2405404 of 6013511 : 1.4674897193908691 seconds \n",
      "Batch 4 : 3006755 of 6013511 : 1.5252783298492432 seconds \n",
      "Batch 5 : 3608106 of 6013511 : 1.5124125480651855 seconds \n",
      "Batch 6 : 4209457 of 6013511 : 1.5285558700561523 seconds \n",
      "Batch 7 : 4810808 of 6013511 : 1.4957430362701416 seconds \n",
      "Batch 8 : 5412159 of 6013511 : 1.506373405456543 seconds \n",
      "Batch 9 : 6013510 of 6013511 : 1.5078237056732178 seconds \n",
      "Total training time: 15.071347713470459 \n",
      "Test dataset accuracy for classifier is 0.7024309247142944 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.00      0.00      0.00    612674\n",
      "Star Rating 1       0.00      0.00      0.00    268689\n",
      "Star Rating 2       0.70      1.00      0.83   2080514\n",
      "\n",
      "     accuracy                           0.70   2961877\n",
      "    macro avg       0.23      0.33      0.28   2961877\n",
      " weighted avg       0.49      0.70      0.58   2961877\n",
      "\n",
      "Total time to test: 7.476577281951904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "n_batches = 10\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train)/n_batches))\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "\n",
    "print(f\"Classifying using SVM\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train[current_batch:next_batch], y_train[current_batch:next_batch], classes=labels)\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "svm_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKKEV3TKoXXx",
    "outputId": "47898c77-a02f-411e-b767-c7f3ab0dc6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using multinomialNB\n",
      "Total time to train: 3.3592495918273926\n",
      "Test dataset accuracy is 0.7024309247142944 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.00      0.00      0.00    612674\n",
      "Star Rating 1       0.00      0.00      0.00    268689\n",
      "Star Rating 2       0.70      1.00      0.83   2080514\n",
      "\n",
      "     accuracy                           0.70   2961877\n",
      "    macro avg       0.23      0.33      0.28   2961877\n",
      " weighted avg       0.49      0.70      0.58   2961877\n",
      "\n",
      "Total time to test: 7.322637319564819\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Classifying using multinomialNB\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Total time to train: {time.time()-start_time}\")\n",
    "start_time = time.time()\n",
    "nb_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy is {nb_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOePhgzCGhXy",
    "outputId": "9dd35b76-9acc-4d9d-83d4-8ae9acefbcdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using KNN\n",
      "Batch 0 : 6013511 of 6013511 : 16.16304087638855 seconds \n",
      "Total training time: 16.1639883518219 \n",
      "Test dataset accuracy for classifier is 0.7024309247142944 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.21      0.95      0.34    612674\n",
      "Star Rating 1       0.09      0.01      0.01    268689\n",
      "Star Rating 2       0.70      0.04      0.08   2080514\n",
      "\n",
      "     accuracy                           0.23   2961877\n",
      "    macro avg       0.33      0.33      0.14   2961877\n",
      " weighted avg       0.54      0.23      0.13   2961877\n",
      "\n",
      "Total time to test: 2217.7434158325195\n"
     ]
    }
   ],
   "source": [
    "n_batches = 1\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train)/n_batches))\n",
    "clf = MiniBatchKMeans(n_clusters=len(labels),random_state=0,batch_size=n_batches)\n",
    "\n",
    "print(f\"Classifying using KNN\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train[current_batch:next_batch])\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "knn_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_Project_Word2Vec_Classes_3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
