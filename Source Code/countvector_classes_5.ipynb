{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCrDIkgtl98y",
    "outputId": "e9e689e6-a539-4e8d-83ac-f3c4b4eb9641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrtxHaAwg_ni"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRJhSCDLbHKO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOsdZnuXhw-r"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSnsk2bzXq_B"
   },
   "outputs": [],
   "source": [
    "#Dataset Link: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "import requests\n",
    "\n",
    "base_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/\"\n",
    "file_name = \"amazon_reviews_us_Wireless_v1_00\"\n",
    "r = requests.get(base_url+file_name+\".tsv.gz\", allow_redirects=True)\n",
    "#Downlaod data from the server to local machine\n",
    "open(file_name+\".tsv.gz\", 'wb').write(r.content)\n",
    "#Unzip downlaoded data\n",
    "with gzip.open(file_name+\".tsv.gz\", 'rb') as f_in:\n",
    "    with open(file_name+\".tsv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "#Remove old zipped data\n",
    "os.remove(file_name+\".tsv\"+\".gz\")\n",
    "#Store data into a pandas dataframe\n",
    "chunk = pd.read_csv(file_name+\".tsv\", sep='\\t',error_bad_lines=False,chunksize=1000000,low_memory=False)\n",
    "df = pd.concat(chunk)\n",
    "#Remove unzipped file from local machine\n",
    "os.remove(file_name+\".tsv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "print(\"Before cleanning reviews\")\n",
    "print(df[\"review_body\"].head(5))\n",
    "def clean_dataset(X):\n",
    "  review = X\n",
    "  stemmer = WordNetLemmatizer()\n",
    "\n",
    "  # Remove all the special characters\n",
    "  review = re.sub(r'\\W', ' ', review)\n",
    "  \n",
    "  # remove all single characters\n",
    "  review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review)\n",
    "  \n",
    "  # Remove single characters from the start\n",
    "  review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n",
    "  \n",
    "  # Substituting multiple spaces with single space\n",
    "  review = re.sub(r'\\s+', ' ', review, flags=re.I)\n",
    "  \n",
    "  # Removing prefixed 'b'\n",
    "  review = re.sub(r'^b\\s+', '', review)\n",
    "  \n",
    "  # Converting to Lowercase\n",
    "  review = review.lower()\n",
    "  \n",
    "  # Lemmatization\n",
    "  review = review.split()\n",
    "  review = [stemmer.lemmatize(word) for word in review]\n",
    "  review = ' '.join(review)\n",
    "\n",
    "  #Removing all  stopwords.\n",
    "  review = remove_stopwords(review)\n",
    "  return review\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X = df[\"review_body\"].apply(lambda x: clean_dataset(str(x)))\n",
    "print(f\"Total time to clean reviews: {time.time()-start_time}\")\n",
    "print(\"After cleanning reviews\")\n",
    "print(X.head(5))\n",
    "y = df[\"star_rating\"]\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "#Saving cleaned data to CSV File\n",
    "X.to_csv(datasetPath+'X_cleaned.csv', index=False) \n",
    "y.to_csv(datasetPath+'y_cleaned.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxZuCRcu9Wxo",
    "outputId": "1ab400e6-fc10-4cc2-cf26-785acdd5572d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load reviews: 30.459144830703735\n",
      "Target Names: [['Star Rating 0', 'Star Rating 1', 'Star Rating 2', 'Star Rating 3', 'Star Rating 4']]\n",
      "(6013513,)\n",
      "(6013513,)\n",
      "(2961880,)\n",
      "(2961880,)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#Reading saved clean file\n",
    "drive.mount('/content/drive')\n",
    "datasetPath = \"/content/drive/My Drive/Natural Language Processing/Project/Cleaned Dataset/\"\n",
    "start_time = time.time()\n",
    "X = pd.read_csv(datasetPath+'X_cleaned.csv')\n",
    "y = pd.read_csv(datasetPath+'y_cleaned.csv')\n",
    "print(f\"Total time to load reviews: {time.time()-start_time}\")\n",
    "\n",
    "#Removing null values\n",
    "result = pd.concat([X,y], axis=1).dropna()\n",
    "X = result[\"review_body\"]\n",
    "y = result[\"star_rating\"]\n",
    "#Cnverting to 5 ratings 0 - 4 from 1 - 5\n",
    "def reset_ratings(r):\n",
    "  if r == 1:\n",
    "    return 0;\n",
    "  if r == 2:\n",
    "    return 1;\n",
    "  if r == 3:\n",
    "    return 2;\n",
    "  if r == 4:\n",
    "    return 3;\n",
    "  if r == 5:\n",
    "    return 4;\n",
    "y = y.apply(lambda x: reset_ratings(int(x)))\n",
    "#Calculating Targets or Classes\n",
    "target_names = []\n",
    "labels = np.unique(y)\n",
    "for label in labels:\n",
    "  target_names.append('Star Rating '+str(label))\n",
    "print(f\"Target Names: {[target_names]}\")\n",
    "#Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train).reshape(X_train.shape[0],)\n",
    "X_test = np.array(X_test).reshape(X_test.shape[0],)\n",
    "#Convert all rating to type int\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_test = np.array(y_test).astype(int)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_pEMrLzexAx"
   },
   "outputs": [],
   "source": [
    "def transform(array):    \n",
    "    vectorizer = CountVectorizer(max_features=100, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "    vectorized = vectorizer.fit_transform(array)\n",
    "    transformer = TfidfTransformer()\n",
    "    transformed = transformer.fit_transform(vectorized)\n",
    "    return np.array(transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8f-H97-aErw",
    "outputId": "97dcf98c-dcad-41aa-bbe8-86f831ddd768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Count Vectors for training data\n",
      "(6013513, 100)\n",
      "Creating Count Vectors for testing data\n",
      "(2961880, 100)\n",
      "Total time to vectorize: 249.68599581718445\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Creating Count Vectors for training data\")\n",
    "X_train = transform(X_train)\n",
    "print(np.array(X_train).shape)\n",
    "print(\"Creating Count Vectors for testing data\")\n",
    "X_test = transform(X_test)\n",
    "print(np.array(X_test).shape)\n",
    "print(f\"Total time to vectorize: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riJC3XhlaIkJ",
    "outputId": "a23bdeb9-76dc-4294-8180-145ce32cedfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting sparse matrix to dense matrix\n",
      "Total time to vectorize: 9.922810316085815\n"
     ]
    }
   ],
   "source": [
    "# convert to sparse matrix to dense matrix\n",
    "start_time = time.time()\n",
    "print(\"Converting sparse matrix to dense matrix\")\n",
    "X_train = csr_matrix(X_train)\n",
    "X_test = csr_matrix(X_test)\n",
    "print(f\"Total time to vectorize: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEUQq1LChH1c"
   },
   "outputs": [],
   "source": [
    "# #scaling vector between 0 - 1 \n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train =scaler.transform(X_train)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_test)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "X_test= X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tZ_U94n3Ltx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9Pgu17_ZNPt",
    "outputId": "e369783f-d827-44c3-a65e-ab15c3998d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using SVM\n",
      "Batch 0 : 601351 of 6013513 : 2.1906371116638184 seconds \n",
      "Batch 1 : 1202702 of 6013513 : 2.043996572494507 seconds \n",
      "Batch 2 : 1804053 of 6013513 : 2.0489237308502197 seconds \n",
      "Batch 3 : 2405404 of 6013513 : 2.021878719329834 seconds \n",
      "Batch 4 : 3006755 of 6013513 : 2.065979242324829 seconds \n",
      "Batch 5 : 3608106 of 6013513 : 2.019120931625366 seconds \n",
      "Batch 6 : 4209457 of 6013513 : 2.0314626693725586 seconds \n",
      "Batch 7 : 4810808 of 6013513 : 2.039930582046509 seconds \n",
      "Batch 8 : 5412159 of 6013513 : 2.00919508934021 seconds \n",
      "Batch 9 : 6013510 of 6013513 : 2.0165042877197266 seconds \n",
      "Total training time: 20.491297483444214 \n",
      "Test dataset accuracy for classifier is 0.5072730833119505 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.24      0.26      0.25    415969\n",
      "Star Rating 1       0.11      0.01      0.02    196705\n",
      "Star Rating 2       0.18      0.02      0.04    268689\n",
      "Star Rating 3       0.38      0.03      0.06    494257\n",
      "Star Rating 4       0.56      0.87      0.68   1586260\n",
      "\n",
      "     accuracy                           0.51   2961880\n",
      "    macro avg       0.29      0.24      0.21   2961880\n",
      " weighted avg       0.42      0.51      0.42   2961880\n",
      "\n",
      "Total time to test: 8.691134929656982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "n_batches = 10\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train)/n_batches))\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "\n",
    "print(f\"Classifying using SVM\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train[current_batch:next_batch], y_train[current_batch:next_batch], classes=labels)\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "svm_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy for classifier is {svm_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKKEV3TKoXXx",
    "outputId": "2b486684-03ad-4c08-c298-4b4ca093d12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using multinomialNB\n",
      "Total time to train: 3.824373722076416\n",
      "Test dataset accuracy is 0.5246603508582387 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.27      0.08      0.13    415969\n",
      "Star Rating 1       0.00      0.00      0.00    196705\n",
      "Star Rating 2       0.25      0.00      0.00    268689\n",
      "Star Rating 3       0.42      0.01      0.02    494257\n",
      "Star Rating 4       0.54      0.95      0.69   1586260\n",
      "\n",
      "     accuracy                           0.52   2961880\n",
      "    macro avg       0.29      0.21      0.17   2961880\n",
      " weighted avg       0.42      0.52      0.39   2961880\n",
      "\n",
      "Total time to test: 8.168979406356812\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Classifying using multinomialNB\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Total time to train: {time.time()-start_time}\")\n",
    "start_time = time.time()\n",
    "nb_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy is {nb_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOePhgzCGhXy",
    "outputId": "20ff9878-0a1c-4725-88d3-9cf5037b450e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using KNN\n",
      "Batch 0 : 6013513 of 6013513 : 17.0790433883667 seconds \n",
      "Total training time: 17.079548597335815 \n",
      "Test dataset accuracy for classifier is 0.144265466528016 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Star Rating 0       0.15      0.50      0.23    415969\n",
      "Star Rating 1       0.05      0.10      0.07    196705\n",
      "Star Rating 2       0.11      0.30      0.16    268689\n",
      "Star Rating 3       0.18      0.13      0.15    494257\n",
      "Star Rating 4       0.46      0.03      0.06   1586260\n",
      "\n",
      "     accuracy                           0.14   2961880\n",
      "    macro avg       0.19      0.21      0.13   2961880\n",
      " weighted avg       0.31      0.14      0.11   2961880\n",
      "\n",
      "Total time to test: 2414.2890326976776\n"
     ]
    }
   ],
   "source": [
    "n_batches = 1\n",
    "current_batch = 0\n",
    "increment = (int(len(X_train)/n_batches))\n",
    "clf = MiniBatchKMeans(n_clusters=len(labels),random_state=0,batch_size=n_batches)\n",
    "\n",
    "print(f\"Classifying using KNN\")\n",
    "start_time0 = time.time()\n",
    "for i in range(n_batches):\n",
    "  start_time = time.time()\n",
    "  next_batch = current_batch + increment\n",
    "  clf.partial_fit(X_train[current_batch:next_batch])\n",
    "  current_batch = next_batch\n",
    "  print(f\"Batch {i} : {current_batch} of {len(X_train)} : {time.time() - start_time} seconds \")\n",
    "print(f\"Total training time: {time.time() - start_time0} \")\n",
    "start_time = time.time()\n",
    "knn_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test dataset accuracy for classifier is {knn_accuracy} \")\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=target_names))\n",
    "print(f\"Total time to test: {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_Project_CountVector_5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
